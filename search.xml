<?xml version="1.0" encoding="utf-8"?>
<search>
  <entry>
    <title><![CDATA[谈谈B-Tree 和 B+Tree]]></title>
    <url>%2F2020%2F02%2F22%2FBTree%2F</url>
    <content type="text"><![CDATA[谈谈B-Tree 和 B+Tree前言周末在看《High Performance Mysql》（中文译为《高性能Mysql》）一书的时候，在index部分看到了B-Tree的概念，实际上这个名词于我而言并不陌生，毕竟之前为了应付面试的时候翻过很多博客，但今天再次看到这个名词时突然回忆不起来B-Tree到底是个什么东西了，于是用度娘去搜了一下，结果搜出来的定义千奇百怪（中文社区里面真的是一堆错误文章抄来抄去，甚至有人说B-Tree是Balance Binary Tree，还假装很懂的说作者是Adelson-Velskii和Landis，看得我一脸懵逼，你这样让AVL-Tree情何以堪？），后面又去外网搜了一下，也发现挺多不一样的定义的，真的是越看越不懂了，后面没办法了就直接找到了原作者R. BAYER and E. MCCREIGHT1971年发布的论文看了一遍，算是真的去了解了一下这个数据结构。 什么是B-Tree关于B-Tree这个名词的说明首先B-Tree这个名词是在R. BAYER and E. MCCREIGHT 1971年发布的一篇文章《Organization and Maintenance of Large Ordered Indexes》中提到的,原文如下： The pages of the index are organized in a special datastructure, so-called B-trees. 看到了吧，虽然B-Tree确实是一颗自平衡的树但是B-Tree的全名就是”B-Tree”,而不是什么Balanced Tree之类的缩写，可以说Balanced Tree包含B-tree但不仅仅只有b-tree ps:其实我更觉得这个B是作者 R. BAYER 的缩写 B-Tree 的定义在文章中，B-tree的定义如下： 假定h为大于等于0的整数，k为一个自然数，那么满足以下条件的有向树T (T ∈ τ(k,h) )就被称为B-Tree： 根节点到所有叶子节点的路径长度都相同，且该路径节点数为h 除根节点和叶子节点之外，每个节点都至少包含k+1个子节点；对于根节点而言要么它同时也是叶子节点，要么至少包含2个子节点 每个节点最多包含2k + 1个子节点 是的你没看错，以上就是B-Tree的完整原始定义，就是这么简洁，没有什么花里胡哨的m阶cell(m/2)之类的东西 B-Tree在索引存储上的使用在上述定义的基础上，当使用B-Tree的节点作为索引储存的页的时候，B-tree还会有以下性质： 除了根页节点能保存的key的个数范围为1~2k之外，其他所有页节点能保存的key的个数的范围为k~2k 假定一个非叶子页节点为P，并且该节点上保存的key的个数为l，那么此节点上会有l+1个子节点 在页节点P 中的所有key都是单调递增的，形如:x0,x1,x2,……,xl;k &lt;= l &lt;= 2k,若P为根节点的话则：1 &lt;= l &lt;= 2k，此外在P中还会包含l + 1个指针用于指向P的子节点，这些指针用p0,p1,……,pl表示。 用于存储索引的一页，同时也是B-Tree的一个节点P的内部结构大概长得像下面这样： 引用一下论文中的图 p0,(x1,p1),(x2,p2),…..(xl,pl) 图中的p0是一个指针，指向所有key都小于x1的节点，随后就是索引里面包含的值x1，然后是x1附带的一些附加信息α1，随后是指针p1指向所有key都大于等于x1，但小于等于x2的节点……,最后是个指针pl，指向所有key都大于xl的节点，了解了这种结构之后就能知道为什么一个节点最多只能保存2k个key:因为指针数会等于子节点数等于2k+1，而key的个数会是指针数-1 B-Tree支持的基本操作查找对于B-Tree来说查找是比较简单的一个操作，我们假定要查的key 为 y，当前页为P 查询过程用语言来描述就是： 二分查找y在 P中是否存在，若存在则直接返回 若y在P不存在，并且P为叶子节点，则返回不存在 若y在P不存在，并且P不为叶子节点，则进入P的子节点，该子节点的指针为pi，满足xi-1 &lt; y &lt; xi+1 插入插入操作的逻辑其实很自然而然就能想到，首先，我们假设需要插入的元素为e，那么在不考虑平衡性的情况下，插入逻辑应该是这样的： 利用上面查找的逻辑，查找当前Btree中有无已存在的e 若有，直接返回插入成功 若无，则在搜索到最后一步的叶子页中插入e 那么上面的逻辑有一个很明显的问题，那就是叶子页的元素个数有可能超过2k，这在Btree的定义中是不被允许的，那么为了解决这个问题，我们就还需要做一步操作：页分裂 页分裂所谓的页分裂就是把一个拥有2k + 1个元素的页进行分裂，产出一个带父节点的新页，分裂之后原来的页中保留前k个元素，分裂出来的页中保留后k个元素，同时分裂前原页中的第k+1个元素会作为分裂出来的页的父节点。 就上述逻辑举个例子，假设原来的叶子节点为P,P中有2k个数据，P的父页为Q，那么该P中的数据存储情况应该是这样的： p0,(x1,p1),(x2,p2),…..(x2k,p2k) 当插入了e之后，现在的P会长这样： p0,(x1,p1),(x2,p2),…..,(xk+1,pk+1),……(x2k+1,p2k+1) 进行页分裂操作之后，P会长这样： p0,(x1,p1),(x2,p2),…..,(xk,pk) 产生的新页我们假设为P’，P’长这样： pk+1,(xk+2,pk+2),(xk+3,pk+3),…..,(x2k,p2k) 同时P’的父节点为： (xk+1,p’) 其中p‘指向的页就是产生的新页P’，随后把此节点再插入P的父页Q中，这样一次完整的页分裂操作就做完了。 所以，完整版的插入逻辑如下： 利用上面查找的逻辑，查找当前Btree中有无已存在的e 若有，直接返回插入成功 若无，则在搜索到最后一步的叶子页中插入e 插入e后叶子页元素个数没有超过2k，插入结束 插入e后叶子页元素个数超过2k，执行页分裂，并向上递归 删除关于删除，不考虑平衡性的话同样很容易就能想到一个朴素的操作办法，流程如下： 利用上面查找的逻辑，查找当前Btree中有无已存在的e 若无，直接返回删除成功 若有，直接删除该元素 那么这个操作流程肯定是有问题的，问题就出在删除元素上，一共有两个问题: 若该元素不在叶子页上，那一旦该元素被删除了，那该元素指向的子页怎么办？ 若该元素在叶子页上，那么一旦叶子页中的元素个数被删至低于k个了，怎么办？ 第一个问题比较好解决，如果该元素不在叶子节点上的话，就将该元素与它的右子节点的第一个元素进行交换，直到交换至叶子节点。 要解决第二个问题的话就需要引入一个新的操作：页合并 页合并页合并就是指把两个兄弟页合并为一个页的操作，具体怎么操作，我们等会用一个例子来说明，现在先约定一些名词，假设页Q中的元素e的左指针指向页P，右指针指向页P’,那么P和P’就被称为兄弟页，页合并操作都是在兄弟页之间进行的，同时页合并也有两种操作，分别被称为Catenation和Underflow CatenationCatenation发生在两个兄弟页中的元素之和小于2K的情况下，这种情况只需要把父页中的元素e给删掉，然后用e作为连接元素将P’中的所有元素合并至P中，随后删除掉页P’,用图表示如下： 合并前： 合并后： 当然父节点中的元素被删掉之后也可能出现元素个数小于k的情况，所以一旦执行了Catenation操作，必须要向上递归执行页合并操作 UnderflowUnderflow发生在两个兄弟页中的元素之和大于等于2k的情况下，这种情况下，我们首先还是先对这两个页做一遍Catenation操作，现在P中的元素个数就一定是大于等于2k + 1了，随后我们再对P执行一遍页分裂操作即可。 虽然页分裂操作会向父页中插入一个值，但是由于在执行页分裂操作前我们就从父页中删除了一个值，所以父页中的元素个数是不会变的，因此Underflow执行一次即可，无需递归执行 B-Tree java代码实现public class BTree { private int k; private Page root; public BTree(int k) { this.k = k; root = new Page(); } public Page getRoot() { return root; } /** * b-tree的节点 */ private class Page { private Element parent; private Element[] elements = new Element[(k + 1) * 2]; private int elementsSize = 1; Page() { elements[0] = new Element(null); elements[0].setIndex(0); elements[0].setPage(this); } public int getElementsSize() { return elementsSize; } public void setElementsSize(int elementsSize) { this.elementsSize = elementsSize; } public Element getParent() { return parent; } public void setParent(Element parent) { this.parent = parent; } public void setElements(Element[] elements) { this.elements = elements; } public Element[] getElements() { return elements; } public boolean addElement(Element value) { int index = bsSearch(elements, 0, elementsSize - 1, value.getV()); return addElement(index + 1, value); } public boolean addElement(int index, Element value) { value.setPage(this); for (int i = elementsSize; i &gt; index; i--) { elements[i] = elements[i - 1]; elements[i].setIndex(i); } value.setIndex(index); elements[index] = value; elementsSize++; return true; } /** * 返回小于等于v的第一个元素 * * @param v * @return */ public Element search(int v) { int index = bsSearch(elements, 0, elementsSize - 1, v); return elements[index]; } public Element getElement(int index) { return elements[index]; } public boolean delete(int index) { for (int i = index; i &lt; elementsSize; i++) { elements[i] = elements[i + 1]; if (elements[i] != null) { elements[i].setIndex(i); } } elementsSize--; return true; } /** * 返回小于等于当前v的第一个元素下标.若不存在则返回-1 * * @param elements * @param v * @return */ private int bsSearch(Element[] elements, int l, int r, int v) { for (; l &lt; r; ) { int mid = (l + r + 1) / 2; if (elements[mid].getV() != null &amp;&amp; elements[mid].getV() &gt; v) { r = mid - 1; } else { l = mid; } } return l; } } /** * 节点中的元素 */ private class Element { private Page rightIndex; private Page page; private int index; private Integer v; public Element(Integer v) { this.v = v; } public Page getRightIndex() { return rightIndex; } public void setRightIndex(Page rightIndex) { this.rightIndex = rightIndex; if (rightIndex != null) { rightIndex.setParent(this); } } public Integer getV() { return v; } public void setV(Integer v) { this.v = v; } public Page getPage() { return page; } public void setPage(Page page) { this.page = page; } public int getIndex() { return index; } public void setIndex(int index) { this.index = index; } } public Element search(int v) { return search(root, v); } public boolean insert(int v) { return insert(root, v); } public boolean delete(int v) { Element vEle = search(v); if (vEle == null) { return false; } // 叶子节点 if (vEle.getRightIndex() == null) { Page currentPage = vEle.getPage(); currentPage.delete(vEle.getIndex()); adjust(currentPage); } else { Page tmpPage = vEle.getRightIndex(); for (; ; ) { Element element = tmpPage.getElement(0); if (element.getRightIndex() != null) { tmpPage = element.getRightIndex(); continue; } element = tmpPage.getElement(1); vEle.setV(element.getV()); tmpPage.delete(element.getIndex()); adjust(tmpPage); break; } } return true; } private Element search(Page node, int v) { if (node == null) { return null; } Element element = node.search(v); if (element.getV() != null &amp;&amp; element.getV() == v) { return element; } return search(element.getRightIndex(), v); } private boolean insert(Page node, int v) { Element vEle = node.search(v); if (vEle.getRightIndex() == null) { node.addElement(new Element(v)); pageSplit(node); return true; } if (vEle.getV() != null &amp;&amp; vEle.getV() == v) { return true; } return insert(vEle.getRightIndex(), v); } /** * 0. 判断当前页是否需要进行页分裂，不需要直接返回 * 1. 将当前页节点 P 中间第k个元素 E(k) 取出 * 2. 新建一个页节点 Pn 将 E(k) 的右指针指向 Pn * 3. 将 P 中的第k+1 ~ 2k + 1个元素移至 Pn 中 * 4. 若 P 为root，则将E(k) 的左指针指向 P ，并新建一个页节点 Pr ，将 E（k）放入 Pr 并将Pr置为为root * 5. 否则 将E(k) 插入P的父页中，递归上述操作 * * @param currentPage */ private void pageSplit(Page currentPage) { if (currentPage.getElementsSize() - 1 &lt;= k &lt;&lt; 1) { return; } Element midElement = currentPage.getElement(k + 1); currentPage.getElements()[k + 1] = null; Page pn = new Page(); for (int i = k + 2; i &lt; currentPage.getElementsSize(); i++) { pn.addElement(i - k - 1, currentPage.getElement(i)); currentPage.getElements()[i] = null; } pn.getElement(0).setRightIndex(midElement.getRightIndex()); midElement.setRightIndex(pn); currentPage.setElementsSize(currentPage.getElementsSize() - k - 1); if (currentPage.getParent() == null) { Page pr = new Page(); pr.getElement(0).setRightIndex(currentPage); pr.addElement(midElement); root = pr; return; } Page parentPage = currentPage.getParent().getPage(); parentPage.addElement(midElement); pageSplit(parentPage); } private void adjust(Page currentPage) { Element parentElement = currentPage.getParent(); // root 就不用管了 if (parentElement == null) { return; } //元素个数大于等于k，也不用管 if (currentPage.getElementsSize() &gt;= k + 1) { return; } Page parentPage = parentElement.getPage(); int left = parentElement.getIndex() - 1 &lt; 0 ? parentElement.getIndex() + 1 : parentElement.getIndex(); int right = parentElement.getIndex() + 1 &gt;= parentPage.getElementsSize() ? parentElement.getIndex() : parentElement.getIndex() + 1; for (int i = left; i &lt;= right; i++) { int leftSize = parentPage.getElement(i - 1).getRightIndex().getElementsSize(); int rightSize = parentPage.getElement(i).getRightIndex().getElementsSize(); if (leftSize + rightSize - 2 &lt; k &lt;&lt; 1) { catenated(parentPage.getElement(i)); adjust(parentPage); return; } if (leftSize + rightSize - 2 &gt;= k &lt;&lt; 1) { underflow(parentPage.getElement(i)); return; } } } private void catenated(Element element) { Page leftPage = element.getPage().getElement(element.getIndex() - 1).getRightIndex(); Page rightPage = element.getRightIndex(); //删除父页中的元素 element.getPage().delete(element.getIndex()); //将删除的元素添加至右页中 rightPage.getElement(0).setV(element.getV()); for (int i = 0; i &lt; rightPage.getElementsSize(); i++) { leftPage.addElement(rightPage.getElement(i)); } } private void underflow(Element element) { Page leftPage = element.getPage().getElement(element.getIndex() - 1).getRightIndex(); Page rightPage = element.getRightIndex(); List&lt;Element&gt; tmp = new ArrayList&lt;Element&gt;(); for (int i = 0; i &lt; leftPage.getElementsSize(); i++) { tmp.add(leftPage.getElement(i)); } rightPage.getElement(0).setV(element.getV()); for (int i = 0; i &lt; rightPage.getElementsSize(); i++) { tmp.add(rightPage.getElement(i)); } //计算出中值索引 int midIndex = tmp.size() / 2; //替换元素值 element.setV(tmp.get(midIndex).getV()); tmp.get(midIndex).setV(null); //更新左页 for (int i = 1; i &lt; leftPage.getElementsSize(); i++) { leftPage.getElements()[i] = null; } leftPage.setElementsSize(1); for (int i = 1; i &lt; midIndex; i++) { leftPage.addElement(i, tmp.get(i)); } //更新右页 for (int i = 0; i &lt; rightPage.getElementsSize(); i++) { rightPage.getElements()[i] = null; } rightPage.setElementsSize(0); for (int i = midIndex; i &lt; tmp.size(); i++) { rightPage.addElement(i - midIndex, tmp.get(i)); } } public static void dfs(Page tmp) { if (tmp == null) { return; } for (int i = 0; i &lt; tmp.getElementsSize(); i++) { if (tmp.getElement(i).getV() != null) { System.out.print(&quot; &quot; + tmp.getElement(i).getV()); } dfs(tmp.getElement(i).getRightIndex()); } } public static void main(String[] args) { BTree bTree = new BTree(2); for (int i = 100; i &gt;= 1; i--) { bTree.insert(i); dfs(bTree.getRoot()); System.out.println(); } for (int i = 1; i &lt;= 25; i++) { System.out.println(bTree.search(i).getV()); } for (int i = 1; i &lt;= 25; i++) { Random random = new Random(); int x = random.nextInt(100) % 100; System.out.print(&quot;delete:&quot; + x + &quot; &quot;); System.out.println(bTree.delete(x)); dfs(bTree.getRoot()); System.out.println(); } } } B+Tree谈完了Btree我们再来谈谈B+Tree，所谓的B+Tree一直没有找到明确的出处和定义，所以这里谈的B+Tree指的是使用最多的定义，也就是mysql中innodb引擎被使用得最多的索引结构 定义B+Tree 的定义与BTree大体上一致，只有一个唯一的不同点，那就是所有的数据都保存在叶子节点中，非叶子节点中只保留索引信息 查询，插入，删除这些操作都同BTree保持一致 innodb索引的页分裂和页合并想了解在实际使用中innodb是如何管理索引树的合并和分裂的，可以查看这篇文章]]></content>
      <categories>
        <category>算法和数据结构</category>
      </categories>
      <tags>
        <tag>B-Tree</tag>
        <tag>B+Tree</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[杂谈之什么是FullGC]]></title>
    <url>%2F2019%2F11%2F14%2Ffullgc%2F</url>
    <content type="text"><![CDATA[本文基于JDK 1.8，使用的收集器为ParNew + CMS 前言这篇文章的起因是这样的，在上周五凌晨很苦逼得参加双十一压测值班的时候，有个业务方突然打电话来说我们提供的客户端存在内存泄漏问题导致线上应用持续full gc,本来已经快要睡着的我立马就精神起来了，一通排查，最终定位到了确实是客户端有个bug会导致部分数据会被一直持有进入老年代之后gc不掉，从而就导致了老年代的频繁gc，具体bug暂且不表，有一个很奇怪的现象引起了我的注意，那就是从监控系统上来看，这个应用平均一分钟full gc次数高达十多次，按照我之前的理解full gc时是会stop the world的，stop the world的频率这么高，那么应用自身的服务已经跪掉了啊，但是看这个应用的业务指标监控，居然一切正常，这就有点超出我的理解能力了，后面为了解决这个疑问，针对什么是full gc，以及如何查看full gc的次数等查阅了很多资料，总算搞懂了full gc这个概念，在查资料的过程中发现中文社区里面包含太多错误的信息了，而且大多都是抄来抄去的，非常误导人，因此打算写一篇文章，对一些错误观点进行纠正。 正文前置知识点在真正开始探索Full GC之前，我们需要先介绍几个概念 GCGC 全称为garbage collection,中文含义为垃圾回收，在jvm中的含义为回收无用内存空间 Young space中文名为年轻代或者新生代，为JVM 堆的一部分，由分代GC概念划分而来，保存生命周期较短的对象 Tenured space中文名为老年代或年老代，为JVM 堆的一部分，由分代GC概念划分而来，保存生命周期较长的对象 Minor GCminor gc指的是发生在年轻代或者说新生代（Young space）中的gc，也有人称其为young gc或者ygc,在下文中我们统一使用minor gc表示 Major GCmajor gc指的是发生在老年代（Tenured space）中的gc，也有人称为old gc,o gc,cms gc等，在下文我们统一使用major gc表示 stop the world指的是用户线程在运行至安全点（safe point）或安全区域（safe region）之后，就自行挂起，进入暂停状态，对外的表现看起来就像是全世界都停止运转了一样,而不论何种gc算法，不论是minor gc还是major gc都会stop the world，区别只在于stop the world的时间长短。 什么是Full GC先说一下结论，Full GC这个概念是没有官方定义的，而且含义还特别混乱，在不同地方表达的含义是不同的，需要就不同的场景分别进行讨论。 大众认知上在通常意义上人们口中说的Full GC为一次特殊GC行为的描述，这次GC会回收整个堆的内存，包含老年代，新生代，metaspace等，这个是最常见的一种认知，很多人也就了解到这个程度，因此在遇到一些特殊场景的时候就会发现实际情况和自己的认知会发生冲突 从GC日志上在gc.log中会发现在部分gc日志头中也有Full GC这样的字眼，这里表示的含义是在这次GC的全过程中，都是Stop The world的状态，也就是说在这次GC的全过程中所有用户线程都是处于暂停的状态，那么在这里要喷一下中文jvm神书《深入理解JVM》了，在第二版第89页有这么一段话： GC 日志开头的“[GC”和“[Full GC”说明了这次垃圾收集的停顿类型，而不是用来区分新生代GC还是老年代GC的。如果有“Full”，说明这次GC是发生了Stop-The-World的，例如下面这段新生代收集器ParNew的日志也会出现“[Full GC”（这一般是因为出现了分配 担保失败之类的问题 .所以才导致STW)。如果是调用System.gc()方法所触发的收集，那么在这里将显示“[Full GC (System)”。 这段话的描述是错误的，因为在前面说过，不论何种gc算法，不论新生代或是老年代，其gc都会发生stop the world，这里正确的描述是这次GC的全过程都是Stop-The-World的 从JDK自带的工具上使用jstat -gc命令能够查看到制定java 线程的gc次数，那么在经过我的多次尝试以及对比之后，我发现了使用jstat 查出来的FGC 次数和时间，实际上指的是老年代的收集器发生Stop the world 的次数和持续时间，对应本文而言，就是CMS收集器的Stop the world次数和时间 其他含义（坑爹版）前面不是说到我在监控大盘上看到这个应用平均一分钟发生了十多次”Full GC”么，在我弄明白了前面两个Full GC的含义之后查看了gc.log文件，随后发现该应用实际上一次Full GC都没有出现，然后咨询了一下提供监控数据的同学，结果他说是通过JMX获取的，代码为： ManagementFactory.getGarbageCollectorMXBeans() 然后众所周知，使用MXBean获取到的只是收集器的执行次数，和Full GC半毛钱关系都没有啊，随后在追问了一下，发现做监控系统的同学直接把CMS收集器的收集次数当做了Full GC的次数来统计，what the fuck？？？？，只能说对于FUll GC的概念真的有很多人是一直没弄懂过的 结束语在这篇文章快写完的时候，用google随便搜了一下就在首页找到一篇对于各类gc概念介绍得比较透彻的文章，和使用度娘一搜首页全是各种抄来抄去的错误文章相比，对比简直是太强烈了，由此得出结论，技术类的资料能在外网查就在外网查吧，省时又省力，最后附上我找到的这篇文章貌似还是plumbr的联合创始人写的，不得不说很给力了]]></content>
      <categories>
        <category>杂谈</category>
      </categories>
      <tags>
        <tag>JVM</tag>
        <tag>Full GC</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Java基础篇 IO系列之File类]]></title>
    <url>%2F2019%2F11%2F09%2FFile%E7%B1%BB%2F</url>
    <content type="text"><![CDATA[本文讨论的JDK版本为1.8.0_181，系统环境为macos FileFile 类的全限定名为 java.io.File,位于rt.jar中，正如《think in java》中介绍的那样，这个类与其叫做File，不如叫做FilePath，因为这个类除了能够表示一个文件之外，还能够表示一个目录，其实这一点也能从源码的第一行注释中看出来，注释如下： An abstract representation of file and directory pathnames 文件和目录地址的抽象表示 在开始分析之前，我们先来看看这个类的构造函数 1. private File(String pathname, int prefixLength) 2. private File(String child, File parent) 3. public File(String pathname) 4. public File(String parent, String child) 5. public File(File parent, String child) 6. public File(URI uri) 总共六个构造函数，其中2个内部的4个公共的，它们的作用都是指定该File类代表的抽象路径，区别在于拼装方式不同，下面就4个公共方法简单介绍一下： public File(String pathname) 将pathname进行标准化处理之后赋值给File类，pathname为此文件或目录的标准路径名字符串,所谓标准路径名就是指不含有重复或多于的分隔符的路径字符串，若pathname中含有多于的分隔符，File类也会把多于的分隔符给过滤掉(例如//User会被处理为/User), 同时pathname可以为相对路径(test/haha)或者绝对路径(/test/haha)，若为相对路径的话，默认的前缀为当前工作目录，也就是System.getProperty(“user.dir”) public File(String parent, String child) 其中的parent表示前缀，只能为路径字符串，child表示后缀，可以为路径字符串也可以为文件名，最终赋值给File类的字符串为 经过标准处理之后的 parent + child public File(File parent, String child) 同上一个方法是一样的，最终赋值给File类的路径为 parent的路径 + child public File(URI uri) 将URL的path赋值给File类，会校验该URL的协议头是否为”file”，若不是会抛出异常 常用方法分析好了，下面开始进入正文，正如先前所说，File类既能表示一个文件也能表示一个目录，那么我们就File类的这两种状态来分别讨论一下 作为文件创建对于文件而言，首要的功能就是创建文件了，毕竟所有的文件操作前提都是你得有个文件File类提供了三个方法用于新建文件，其中一个内部方法，两个静态方法： 1. public boolean createNewFile() 2. public static File createTempFile(String prefix, String suffix) 3. public static File createTempFile(String prefix, String suffix,File directory) public boolean createNewFile() 比较简单，主要作用就是使用创建File类时传进来的路径来创建文件，若该路径中包含的目录不存在则会直接抛异常，若该文件已经存在了则不进行任何操作 public static File createTempFile(String prefix, String suffix)底层调用的也是public static File createTempFile(String prefix, String suffix,File directory),因此我们只需要看一下public static File createTempFile(String prefix, String suffix,File directory)就好了，此方法是一个静态方法，主要作用是用来创建一个临时文件，其中prefix为文件的前缀名，suffix为文件的后缀名，directory为目录，其中前缀必须为长度大于3的字符串，并且只取文件名（例如若传入的前缀为/user/test,那么最终取的前缀为test），后缀若未传则默认为”.tmp”, directory若未传的话默认为系统临时目录，即 System.getProperty(“java.io.tmpdir”); 读写创建了文件之后，我们当然会想要来对文件进行一下读写了,但File类本身并不提供读写功能，对于Fille的读取，我们是通过后面将会提到的字节流和字符流来实现的。 作为文件的特殊情况我们已经讨论完了，接下来再看一下作为目录的情况 作为目录目录特有的操作比较简单 创建创建目录共有两个方法，分别为： 1. public boolean mkdir() 创建当前目录，若父目录不存在，则创建失败若成功创建则返回true，否则返回false 2. public boolean mkdirs() 创建当前目录，若父目录不存在则先创建父目录，若成功创建则返回true，否则返回false 遍历目录目录的遍历是作为目录而言最常用的方法，一共有5个，分别如下： 1. public String[] list() 返回当前目录下所有文件或目录名称字符串数组（不会递归遍历） 2. public String[] list(FilenameFilter filter) 返回当前目录下符合filter过滤条件的所有文件或目录名称字符串数组，其中FilenameFilter是一个接口，提供了accept方法可用于重写（不会递归遍历） 3. public File[] listFiles() 查找当前目录下所有文件或者目录，并以File类的形式返回（不会递归遍历） 4. public File[] listFiles(FilenameFilter filter) 同理，会只返回filter过滤后的File类文件 5. public File[] listFiles(FileFilter filter) 查找当前目录下所有文件或者目录，并以File类的形式返回，会只返回filter过滤后的File文件，这里的FileFilter与FilenameFilter的区别在于FilenameFilter的入参是String，而FileFilter的入参为File类 公有方法作为文件和目录都有的方法： 1. public boolean canRead() 是否可读 2. public boolean canWrite() 是否可写 3. public boolean exists() 判断当前目录或文件是否存在 4. public boolean isDirectory() 判断当前是否为目录 5. public boolean isFile() 判断当前是否为文件 6. public boolean isHidden() 是否为隐藏文件 7. public boolean delete() 删除文件或者目录，立即删除 8. public void deleteOnExit() 删除文件或者目录，当java进程退出时执行 9. public boolean renameTo(File dest) 将文件或者目录改名为传入的File类的pathName，若目标目录或者文件已存在，则返回失败]]></content>
      <categories>
        <category>Java基础</category>
      </categories>
      <tags>
        <tag>File类</tag>
        <tag>IO</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[关于flag的那些事]]></title>
    <url>%2F2019%2F10%2F04%2F%E7%AB%8B%E4%B8%AAflag%E5%90%A7%2F</url>
    <content type="text"><![CDATA[周末就喜欢瞎想，虽然还有一大堆事情没有做 模拟退火与布朗运动前段时间看到了一个挺有意思的观点： 人生的优化问题可以通过数学的概念抽象为如下两点 1. 以你想成为什么样的人作为目标函数 2. 将你必须要做的事作为约束条件 上述两个条件的有无，就决定了你目前的状态到底是做的模拟退火还是布朗运动 乍一想，貌似还有那么点道理，想了一下毕业这三年来自己的一个状态，可以说算是典型的布朗运动了，每一个对未来有重大意义和影响的决定，都没有任何理由和思考，都是在周围环境或他人的影响下随波逐流，没有想清楚过自己做的这些决定与完成自己的终极目标方向是否一致，包括刚毕业时拿到offer去上海，其实那个时候根本没有想过为什么要去上海，去上海想要得到些什么，只是当时莫名地觉得去上海发展会好一点（其实也是周围人的说法，与自己而言没什么感触），然后就是最近跳槽到杭州，我又为什么要来杭州，来杭州之后我又想要得到些什么，做这些事能够帮助我实现我的最终目标吗？这些都没有想过，就像一个小分子，做着无规律的布朗运动 想一想，今年马上就25了，已经一把年纪了，是时候弄清楚自己到底想要些什么了，生活还是需要有个方向的。 梳理一下首先，就个人来说，终极目标就三个，第一个是健康的身体，第二个是充足的财富，第三个是足够的时间，相信这也是大多数人所想要的，接下来再按照这三个目标进行一下细分，梳理一下约束条件。 健康的身体要拥有一个健康的身体，除了一些不可抗因素之外，坚持做到三个方面应该就OK了 1. 良好的作息时间 2. 良好的饮食习惯 3. 合理的锻炼时间 就上述三个方面，分别制定一条约束条件 1. 拒绝熬夜，每天11点前睡觉，6~7点钟起床 2. 每日三餐，按时吃饭，避免暴饮暴食，过度油腻 3. 每日半小时锻炼时间，可跑步或其他 充足的财富那么有多少财富算是充足的财富呢，答案是再多也不够，未来会发生什么我们永远也不知道，可能现在觉得完全足够的钱在未来会变得远远不够，现在觉得非常稳定的工作到未来也可能在眨眼之间失业，所以要保证有充足的财富个人认为应该做到有这两方面 1. 保持自己的竞争力，个人成长必须跟上以及超过时间的流逝，不论何时，都能够靠个人能力赚到足够生活的钱，可以为大公司打工，也可以是作为自由职业者，总之就是能够通过劳动换来报酬。 2. 在不工作的情况下也能有源源不断的现金流，换句话说就是要有真正属于自己的事业，或者说资本，然后用资本为自己赚钱。 只要能够做到上述两点，那么随着时间的过去，总会积累下足够的财富，因为这是一个良性循环的过程。 那么就这两个方面，我们来看一下约束条件 1. 对于保持竞争力这一点，其实说起来很简单，就一句话：持续学习，确定好自己从事的专业方向，然后保持每天或是每周都花费足够的时间学习这个方向的相关知识，或是打磨技能，就能够一直保持自己的竞争力，当然这里的持续是真正意义的持续，不能因为任何原因停止 2. 对于资本，目前自己也不清楚如何建立自己的资本，唯一能想到的就是不断拓宽自己的眼界，寻找值得投资的机会 足够的时间足够的时间就是指能够有时间做自己真正想要做的事，比如，每天去一下健身房，去一下英语培训班，或者音乐班，以及和三五好友聚会，独自去旅行等等。这一项是最难以实现的，因为通常财富都是在牺牲时间的前提下换来的，举个例子，对于小码农来说，每天早上起床就是上班，下班回家之后就是睡觉，所有的时间全部在公司里面，已经和整个社会脱节了，像很普通的下班之后看个电影吃个饭，逛逛商场之类的活动对于程序员(中国的)来说是遥不可及的，因为下班的时候绝大部分店都已经关门了，因此对于程序员这类用全部时间换钱的职业来说，是不可能有充足的时间的。 那么要做到这一项，我们都些什么呢 1. 其实我们只需要有足够的财富，也就是得先实现上一个目标，并且这个实现得是建立在构建了自己的资本之后，因为为别人打工的情况下，老板总是想要花最少的钱，买你最多的时间，哪怕多出来的这些时间对他来说没有任何价值（这是因为中国大部分管理者管理能力都很低下，只把员工当做劳动密集型工种在使用），有了自己的资本之后，就会有很多人花他们的时间来为你赚钱，而你自然就会有足够的时间 那么这一条的约束条件是什么呢 其实同上一条的约束条件类似，就是一定要拥有自己的资本，就现状而言，我们可以努力提升自己的竞争力，拓宽自己的眼界，同时学会理财，积累好自己的原始资本，在有机会的情况下，毫不犹豫地去创建自己的资本 结束语梳理了这么一遍之后，为自己制定了一个新的计划，首先，按照第一个约束条件做到良好的作息时间和锻炼等，这是所有flag的前提，需要一直坚持下去，其次，平时立的一些短期flag(例如每年的计划表，每月计划表等)需要将最终目标设置为提高自己的竞争力，这样就能满足第二条约束条件，最后，多接触新鲜事物，多思考，多结识新的朋友，这样就能满足最后一条约束条件]]></content>
      <categories>
        <category>杂谈</category>
      </categories>
      <tags>
        <tag>随想</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[个人对高并发服务的理解]]></title>
    <url>%2F2018%2F08%2F16%2F%E6%80%BB%E7%BB%93%2F</url>
    <content type="text"><![CDATA[总结一下 前言本文主要是简单介绍一下一个服务由单机部署到分布式部署，访问流量从低到高，这个过程中，我们所使用到的相关技术，以及需要主要到的点 由于本人水平极其有限，因此有什么错误的地方(感觉会有很多)欢迎大家指出 一些概念： 多核cpu以及超线程技术： 逻辑cpu核数: 物理cpu数 * cpu 核数 * 超线程数 cpu的load average：即cpu当前负载，举个栗子，若一个cpu1分钟能够处理的进程数是100，然后在这一分钟内只有20个进程需要被处理，那么当前cpu的负载为20/100 = 0.2通常一台机器的load average不应当超过0.7 * 逻辑cpu核数，当超过了就应当去查找原因，要是达到 1.0 * 逻辑cpu核数 那么就必须要想办法降下来 QPS(Query Per Second):即每秒查询率,是对一个特定的查询服务器在规定时间内所处理流量多少的衡量标准 TPS(TransactionPerSecond):即每秒事务处理量，指每秒钟系统能够处理的交易或事务的数量 提高单机性能-多线程技术：多线程是否一定能提升效率：首先需要明确一点，多线程只是将一个大的计算任务分为了多个并行的小的计算任务，这些拆分出来的任务还是需要分别去抢占cpu时间片，总体需要花费的时间并没有减少，只是使得任务可以并行执行 因此多线程能够提升程序执行效率的一个关键点是这台机器上的cpu，如果是单核的话，多线程反而比单线程慢因为增加了上下文切换的开销，同时对于多核cpu来说线程数也不是越多越好，举个例4核8线程的cpu 能够同时处理的任务也就8个，起再多线程也没用，反而会增加不必要的开销，通常来说保证同时处理的 线程数 = N(CPU核心数) + 1 能使系统效率达到最优，当然这里是指计算密集型线程，如果是io密集型的话可以适当增加线程数因为io线程并不一定会去抢占cpu 关于线程安全：什么是线程安全，感觉说法很多，个人理解线程安全问题就是你写的代码在多线程的情况下执行结果没有达到期望的目标比如hashmap在多线程的场景下在resize的时候有一定几率导致死循环，以及多个线程同时对同一个变量进行compare and set操作等总结了一下能产生线程安全的一个必要条件就是先得有多个线程同时访问并操作同一个数据，所以我们在写代码的时候需要注意当定义了一个static 类型的变量，或者是类成员变量时需要考虑是否会出现线程安全问题如果有可能出现线程安全问题，首先可以考虑一下用threadLocal能否解决问题，如果不能则需要考虑合理使用锁（不论是synchronized关键字还是lock类），但是使用锁的时候也需要注意是否会出现两个线程循环依赖，导致出现死锁的情况。 集群化服务-分布式部署：所谓分布式服务，个人理解就是服务和资源分散部署到不同的机器上使用原因是因为单机部署存在一个机器性能瓶颈，以及单点问题，所以线上所有服务都是多机部署的。当线上的流量变大了之后，以及提供服务的机器多了之后，如何去分发这些流量，以及流量都应该分发到哪台机器上这就是负载均衡解决的问题 流量转发-负载均衡：负载均衡分为4层负载均衡和7层负载均衡，其中4层负载均衡作用于OSI模型的第四层传输层，这一层主要的协议是tcp和udp，由此可以知道这一层负载均衡的作用主要是对数据包的转发通常客户端的流量过来了之后到达4层负载均衡，然后4层负载均衡通过修改数据包的地址信息将流量转发到应用服务器 7层负载均衡的话作用于OSI模型的第7层应用层，主要工作为代理，在将应用层的流量完整解析出来之后，7层负载均衡会新建一个连接到实际需要访问的机器上，将解析出的请求投递过去 公司现状：4层负载均衡使用的自研mgw，7层(web端)用的nginx或者Tengine，然后实际到service端的时候是使用的rpc服务自带的负载均衡附上一篇文章 MGW RPCRPC服务主要是为了简化部署在不同机器上的不同服务之间的相互调用不多讲，甩篇文章先 RPC 数据量日益增加-分库分表：现在线上提供服务的机器已经由一台变成多台了，并且能够水平扩展，因此在某种程度上已经解决了高访问量的问题(其实并没有)加机器就好了嘛，但接下来还是会遇到各种各样的问题首先一个比较容易想到的是当服务在线运行时间比较久或者访问量比较大之后，通常会导致数据库中或者表中的数据量越来越大，当数据量大了之后不论是插入还是查询都会变慢长此以往，直接就会导致我们的服务不可用,并且这种场景下扩几台应用机都没用，那么机智的小伙伴肯定想到了，扩应用机没用，扩DB不就行了么，答案确实是这样的，只不过我们用了一个比较高端的名词–分库分表所谓分库分表，其实就是将一个库或者表的数据分散到不同的库，或者不同的表中，然后通过提前定义好的分库规则和分表规则去访问不同库表，降低单库单表访问压力提高读写性能(ps:其实分库分表的前置技术栈还有一个读写分离技术，篇幅有限，就不说了，大致提一句就是单库写，多库读) 一些关注点分库分表的临界值 单表数量不超过1000万 单库数据量不超过300G 分库分表之后会遇到的问题 基本的数据库增删改功能 分布式id 分布式事务 动态扩容 使用zebra进行分库分表： 最后甩个链接 zebra 读多写少-缓存的使用：当把数据量的问题解决了之后，随着业务的发展服务访问量日益增加，你会发现读写分离+分库分表能处理的请求量(这里指qps或Tps，不过通常都是读压力比较大)还是有限，这就是所谓的DB瓶颈针对这种情况一个常用的优化措施就是缓存，利用缓存的高吞吐量的特性，在db前部署一层缓存可以大大降低db的负载，同时通常缓存响应的速度比db快(内存缓存)，因此在提高吞吐量的情况下还提高了访问速度，简直妙啊 redis和tair介绍:redis 纯内存K-V缓存，支持5种数据结构String、Hash 、List 、 Set 、 ZSet tair 淘宝开源 内存结构主要看存储引擎选择，例如MDB，LDB等 具体可见jiajun大佬的分享: 大佬的分享Redis AND 大佬的分享Tair 使用缓存需要注意的问题 缓存穿透通常在缓存未命中的情况下我们回去db再查一次数据，要是有人使用不存在的key来请求，那么流量就相当于直接落在db上 解决方案：访问db前进行合理性校验 缓存击穿单个key在过期的一瞬间被大量请求访问 解决方案：在访问db前，先加个互斥锁，例如setnx 缓存雪崩批量key在同一时间过期，大量请求直接到达db，然后db负载飙升，引起雪崩 解决方案：采用不同的过期时间，例如在固定过期时间的基础上加上一个随机值 不必要流程的异步化-MQ：那么在原有的系统上加上一层缓存之后就可以高枕无忧了么，答案当然是NO，下面列举一些场景： 写流量非常高 缓存通常只被用来解决读性能问题，最终数据一致性还是要靠db来保证(其实就是要把数据刷到磁盘上)，因此不可能将所有的写都放到缓存上 瞬时流量暴增 系统流量在短时间内倍增，例如节日或者活动期间，突发流量很有可能将缓存直接压垮，然后系统GG 针对这些问题，乍一看似乎是没什么好的办法可以解决了(其实有，但是我不知道)，不过针对某些特殊的场景我们还是可以优化一下的比如： 我如果能减少每个请求的处理时间，那么是不是就能够提高我单机的qps，那么瞬时流量来的时候是不是就不至于压夸整个系统了 对于写请求的话如果对于响应时间的要求不是那么高，那么先把这些请求都保存下来，然后慢慢处理不就好了 针对第一种优化方案，我们能想到的一个点是将系统流程中的一些不那么重要的分支流程由同步改为异步，例如更新缓存的操作然后第二种优化方案其实也是一种异步化的思想，将同步的事改为异步去做，只要保证最终一致性就好了。 那么怎么来实现这种异步化操作呢，这个时候就轮到MQ(MessageQueue)大佬出场了。 所谓MQ，其实就是一个生产者-消费者模型，producer负责生产消息，然后consumer消费消息，中间还有一个broker用来保存生产的消息像我们刚才说的这些场景我们可以让我们的服务器既作producer又做consumer，将需要异步处理的任务统统先发送到broker,然后慢慢取回来消费当然MQ也并没有我说的这么简单，篇幅有限加上个人能力有限我这里就不细说了，具体可以看看我司大佬的博文 这个是大佬文章 kafka介绍:我司使用的MQ是mafka，其实就是封装了一下kafka，针对kafka篇幅有限加上个人能力有限也不细说了，简单介绍一下 提升可用性-限流和熔断降级：当系统的可用性依赖与其他服务的可用性时，熔断降级机制是提升系统本身可使用性的一大利器同时针对突发流量，限流工具的使用也是相当有必要的 rhino介绍：Rhino]]></content>
  </entry>
  <entry>
    <title><![CDATA[java程序员基本素养(二) 更复杂的java程序]]></title>
    <url>%2F2018%2F07%2F19%2Fjava%E7%A8%8B%E5%BA%8F%E5%91%98%E5%9F%BA%E6%9C%AC%E7%B4%A0%E5%85%BB2%2F</url>
    <content type="text"><![CDATA[何如使用中间件，工具等只是术的层次，我们更需要关注背后的道的层次 io要写功能更强大的程序，不可避免要涉及io，不论是从文件中获取还是从网络中获取 java.io 库 待填坑 bio、nio、aio 对比 待填坑 我司大佬写的NIO 网络编程除非你一直都是写单机程序自娱自乐，不然我们总会需要将自己的服务发布到网络上 java网络编程 待填坑 多线程现在的计算机通常都不止一个CPU，同时由于CPU的超线程技术，可以使一个CPU同时处理两个线程，使得计算机的计算能力大幅提升为了更好地压榨机器的性能，多线程编程技术必不可少 Java多线程 待填坑 ThreadPoolExecutor 我写的]]></content>
      <categories>
        <category>杂谈</category>
      </categories>
      <tags>
        <tag>随想</tag>
        <tag>java程序员基本素养</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[java程序员基本素养(一) 从java文件到在JVM中运行]]></title>
    <url>%2F2018%2F07%2F12%2Fjava%E7%A8%8B%E5%BA%8F%E5%91%98%E5%9F%BA%E6%9C%AC%E7%B4%A0%E5%85%BB%2F</url>
    <content type="text"><![CDATA[个人理解的作为一个java程序员应当有的基本素养系列，只维护一个目录作为查缺补漏用 .java 文件java 码农生涯的起点是从一个.java文件开始的 java语法 待填坑 .class 文件.java 文件通过javac编译后每个类都会生成一个.class文件.class文件是JVM生态圈里比较重要的一部分，也是java实现跨平台部署的基础 .class 文件结构 待填坑 字节码指令含义 待填坑 这个不错 java语法如何转换为字节码(不那么重要) 待填坑 .class 文件加载进内存中要使用.class文件先得把.class文件加载到内存中 类加载机制 待填坑 内存中的.class文件如何转化为机器码字节码还是能被JVM识别，但是正在进行运算的还是机器本身，所以需要把字节码转化为可被机器识别的机器码 解释器编译器，JIT等 待填坑 GC程序运行起来之后，在运行过程中需要不断地申请和释放内存，怎么来分配和回收内存就需要我们关注一下了，我们通常使用的VM都是hotspot VMhotspot VM为我们提供了很多的垃圾收集器 JVM 内存分配方案(hotSpot) 待填坑 JVM 收集器(hotSpot) 待填坑 Java 常用调试工具(jsp,jmap,jstack等) 待填坑]]></content>
      <categories>
        <category>杂谈</category>
      </categories>
      <tags>
        <tag>随想</tag>
        <tag>java程序员基本素养</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[innodb为什么要使用B+树做索引]]></title>
    <url>%2F2018%2F06%2F21%2Finnodb%E4%B8%BA%E4%BB%80%E4%B9%88%E8%A6%81%E4%BD%BF%E7%94%A8B-%E6%A0%91%E5%81%9A%E7%B4%A2%E5%BC%95%2F</url>
    <content type="text"><![CDATA[mysql 提供的存储引擎有多种，目前我只用过innodb，因此就只谈论innodb 一个疑问innodb 是采用多个辅助索引和一个聚簇索引的存储结构，其中辅助索引和聚簇索引都是采用B+树的结构来进行存储的唯一的区别在于辅助索引的叶子节点存储的是主键，而聚簇索引的叶子节点存储的是数据，那么为什么要采用B+树这种结构来存储呢，有什么好处么 个人想法 树形结构可以有效降低查询和插入效率基本维持在常数级O(logn) 对于从硬盘中读取数据的操作而言，不论是传统的机械磁盘还是ssd，随机io的开销总会大于顺序io基于内存预加载机制，及每次从磁盘加载数据到内存时总会将相邻数据一并加载到内存，通常加载到内存的数据大小为一页，若将B+树一个节点的大小设置为一页那么，查找一条记录需要进行的IO操作次数就刚好是B+树的高度，刚好B+树的高度是高度可控的。]]></content>
      <categories>
        <category>杂谈</category>
      </categories>
      <tags>
        <tag>随想</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[RPC原理]]></title>
    <url>%2F2018%2F06%2F02%2FRPC%E5%8E%9F%E7%90%86%2F</url>
    <content type="text"><![CDATA[原理讲解网上一大篇就不复制粘贴了 贴一篇自己觉得讲得比较清楚的 RPC原理 个人总结RPC是为了简化部署在不同机器上的不同服务之间的相互调用，使用生成动态代理类的方式，使得调用远程服务和调用本地服务并无区别，其中比较重要的点有 JDk的动态代理 服务的发布和服务不可用时下线，以及客户端如何查找可用服务等,通常使用zk实现，一个节点为一台机器 客户端和服务端之间的通信协议和报文，通常使用自定义报文,tcp协议，网络传输使用Netty通过NIO的方式通信 序列化方式的选择多种，我司默认使用hessian序列化]]></content>
      <categories>
        <category>中间件</category>
      </categories>
      <tags>
        <tag>日常学习</tag>
        <tag>RPC</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Threadpoolexecutor源码学习]]></title>
    <url>%2F2018%2F06%2F02%2FThreadpoolexecutor%E6%BA%90%E7%A0%81%E5%AD%A6%E4%B9%A0%2F</url>
    <content type="text"><![CDATA[问题 Threadpoolexecutor 各个参数的含义 线程的创建销毁策略 如何实现 使用场景通常使用JDK的executor框架来构建线程池和提交任务 ExecutorService executorService = Executors.newFixedThreadPool(10); executorService.submit(new Callable&lt;String&gt;() { @Override public String call() throws Exception { dosomething(); return null; } }); 在executor框架中，ExecutorService有多个实现类，本文主要介绍的也是我们最常使用的为ThreadPoolExecutor 参数说明构造函数： public ThreadPoolExecutor(int corePoolSize, int maximumPoolSize, long keepAliveTime, TimeUnit unit, BlockingQueue&lt;Runnable&gt; workQueue, ThreadFactory threadFactory, RejectedExecutionHandler handler) { if (corePoolSize &lt; 0 || maximumPoolSize &lt;= 0 || maximumPoolSize &lt; corePoolSize || keepAliveTime &lt; 0) throw new IllegalArgumentException(); if (workQueue == null || threadFactory == null || handler == null) throw new NullPointerException(); this.corePoolSize = corePoolSize; this.maximumPoolSize = maximumPoolSize; this.workQueue = workQueue; this.keepAliveTime = unit.toNanos(keepAliveTime); this.threadFactory = threadFactory; this.handler = handler; } corePoolSize 核心线程数,线程池内恒定线程数量 maximumPoolSize 最大线程数，线程池内运行存在的最大线程数 keepAliveTime 非核心线程的过期时间 unit 过期时间的单位 workQueue 用于存放任务的工作队列 threadFactory 线程工厂，用于生产线程 handler 拒绝策略，当线程池中线程达到最大数量时，对新提交的任务的拒绝策略 类结构图 ThreadPoolExecutor 继承于 AbstractExecutorService，实现了Executor以及ExecutorService接口因此同时包含submit和execute方法，我们先从最核心的execute方法来进行分析 源码分析execute方法源码如下： int c = ctl.get(); if (workerCountOf(c) &lt; corePoolSize) { if (addWorker(command, true)) return; c = ctl.get(); } if (isRunning(c) &amp;&amp; workQueue.offer(command)) { int recheck = ctl.get(); if (! isRunning(recheck) &amp;&amp; remove(command)) reject(command); else if (workerCountOf(recheck) == 0) addWorker(null, false); } else if (!addWorker(command, false)) reject(command); 其中比较重要的一个变量是ctl,这是一个AtomicInteger变量，其中高3位表示线程池状态，低29位表示线程数量我们继续看源码，假设我们现在的场景是刚刚初始化了一个线程池，核心线程数为3，最大线程数为5 首先是调用workerCountOf方法来获得当前线程数，我们现在是第一次提交任务，因此当前线程数为0，小于核心线程数3，因此会执行addWorker方法我们继续debug到addWorker方法里，看看都做了些什么操作。 addWorker一共做了两个操作，我们先来看看第一部分 step 1:对ctl做cas操作增加线程数 retry: for (;;) { int c = ctl.get(); int rs = runStateOf(c); // Check if queue empty only if necessary. if (rs &gt;= SHUTDOWN &amp;&amp; ! (rs == SHUTDOWN &amp;&amp; firstTask == null &amp;&amp; ! workQueue.isEmpty())) return false; for (;;) { int wc = workerCountOf(c); if (wc &gt;= CAPACITY || wc &gt;= (core ? corePoolSize : maximumPoolSize)) return false; if (compareAndIncrementWorkerCount(c)) break retry; c = ctl.get(); // Re-read ctl if (runStateOf(c) != rs) continue retry; // else CAS failed due to workerCount change; retry inner loop } } 首先先检查了一下当前线程池的状态，若当前状态不接受新的work则直接返回false 然后就是检查当前线程数是否超过限制，对于核心线程来说是不能超过核心线程数也就是3，对于其他线程来说是不能超过最大线程数，也就是5 然后就是cas操作，增加线程数，若成功，则跳出循环 操作3在并发情况下有可能出现操作失败的场景，若失败则先检查一下当前线程池状态是否发生变化，若变化了则跳到步骤1若状态未发生变化则跳到操作2 在成功添加线程数之后就开始真正地添加任务了 step 2 添加worker boolean workerStarted = false; boolean workerAdded = false; Worker w = null; try { final ReentrantLock mainLock = this.mainLock; w = new Worker(firstTask); final Thread t = w.thread; if (t != null) { mainLock.lock(); try { // Recheck while holding lock. // Back out on ThreadFactory failure or if // shut down before lock acquired. int c = ctl.get(); int rs = runStateOf(c); if (rs &lt; SHUTDOWN || (rs == SHUTDOWN &amp;&amp; firstTask == null)) { if (t.isAlive()) // precheck that t is startable throw new IllegalThreadStateException(); workers.add(w); int s = workers.size(); if (s &gt; largestPoolSize) largestPoolSize = s; workerAdded = true; } } finally { mainLock.unlock(); } if (workerAdded) { t.start(); workerStarted = true; } } } finally { if (! workerStarted) addWorkerFailed(w); } 可以看到首先是将task做为入参new了一个Worker的对象,我们先去看看Worker对象的构造函数 Worker(Runnable firstTask) { setState(-1); // inhibit interrupts until runWorker this.firstTask = firstTask; this.thread = getThreadFactory().newThread(this); } Worker是ThreadPoolExecutor的一个内部类继承了AbstractQueuedSynchronizer实现了Runnable接口构造函数做的事情就是将aqs的state置为-1，并使用我们提供的线程工厂创建一个线程，然后这里需要注意的一点是这里调用newThread方法传的参数是this，也就是说创建的所有线程实际运行的时候都是执行的Worker类的run方法 接下来就是将Worker对象添加进workers中，在添加前同样会先判断当前线程池的状态是否可以添加线程另外由于workers是一个hashSet，hashSet底层又是调用的hashmap因此是线程不安全的，所以此处还使用了一个全局的ReentrantLock来对添加方法加锁 如果添加线程成功的话，则会启动该线程，若失败的话则会调用addWorkerFailed方法，addWorkerFailed做的事就是把Worker对象从workers中移除，并将当前线程数减1同时调用tryTerminate尝试终止线程池。 tryTerminate方法主要做的事情有: 若当前线程池的状态不可终止，则不做任何事 若线程池的状态为可以终止状态，但工作队列不为空，则interrupt掉workers中的一个线程 若线程池的状态为可以终止状态且工作队列为空，将当前线程池状态置为TERMINATED 前面说到了启动线程其实是调用Worker的run方法，我们去看看run方法里面到底做了什么 run调用的是父类中的runWork方法，代码如下 Thread wt = Thread.currentThread(); Runnable task = w.firstTask; w.firstTask = null; w.unlock(); // allow interrupts boolean completedAbruptly = true; try { while (task != null || (task = getTask()) != null) { w.lock(); // If pool is stopping, ensure thread is interrupted; // if not, ensure thread is not interrupted. This // requires a recheck in second case to deal with // shutdownNow race while clearing interrupt if ((runStateAtLeast(ctl.get(), STOP) || (Thread.interrupted() &amp;&amp; runStateAtLeast(ctl.get(), STOP))) &amp;&amp; !wt.isInterrupted()) wt.interrupt(); try { beforeExecute(wt, task); Throwable thrown = null; try { task.run(); } catch (RuntimeException x) { thrown = x; throw x; } catch (Error x) { thrown = x; throw x; } catch (Throwable x) { thrown = x; throw new Error(x); } finally { afterExecute(task, thrown); } } finally { task = null; w.completedTasks++; w.unlock(); } } completedAbruptly = false; } finally { processWorkerExit(w, completedAbruptly); } 去掉一些不重要的逻辑我们可以看到runWork方法就是用当前线程不停地去取task，然后调用task的run方法task可以是一开始创建这个work时初始化的，也可以是调用getTask方法获取到的 getTask方法如下： boolean timedOut = false; // Did the last poll() time out? retry: for (;;) { int c = ctl.get(); int rs = runStateOf(c); // Check if queue empty only if necessary. if (rs &gt;= SHUTDOWN &amp;&amp; (rs &gt;= STOP || workQueue.isEmpty())) { decrementWorkerCount(); return null; } boolean timed; // Are workers subject to culling? for (;;) { int wc = workerCountOf(c); timed = allowCoreThreadTimeOut || wc &gt; corePoolSize; if (wc &lt;= maximumPoolSize &amp;&amp; ! (timedOut &amp;&amp; timed)) break; if (compareAndDecrementWorkerCount(c)) return null; c = ctl.get(); // Re-read ctl if (runStateOf(c) != rs) continue retry; // else CAS failed due to workerCount change; retry inner loop } try { Runnable r = timed ? workQueue.poll(keepAliveTime, TimeUnit.NANOSECONDS) : workQueue.take(); if (r != null) return r; timedOut = true; } catch (InterruptedException retry) { timedOut = false; } } 这个方法做的事就是从一个阻塞队列中获取值，其中比较重要的一些状态的判断逻辑 当前线程池状态为SHUTDOWN并且workqueue为空，或者当前线程池状态为STOP时直接返回null 若当前线程数大于了最大线程数，减少一个线程数，并返回null 如若当前线程数大于核心线程数或者设置了允许核心线程数过期，同时从workqueue队列中取数据超时，返回null，减少一个线程数 至此，当当前线程数小于核心线程数时的全部流程我们走过一遍了，接下来我们再回到一开始的execute方法，看看当前线程数大于核心线程数时的策略 if (isRunning(c) &amp;&amp; workQueue.offer(command)) { int recheck = ctl.get(); if (! isRunning(recheck) &amp;&amp; remove(command)) reject(command); else if (workerCountOf(recheck) == 0) addWorker(null, false); } else if (!addWorker(command, false)) reject(command); 若当前线程池的状态为RUNNING并且，向workQueue中添加任务成功，则进行recheck操作 recheck操作主要是防止添加任务之后线程池状态变更，以及在设置了过期时间之后，当前线程全部过期的场景 如果添加任务失败，或者出现线程池状态变更，则进行reject操作 reject操作实际上就是调用我们自定义的或者默认的RejectedExecutionHandler 总结上面我们已经把线程池的核心代码全部梳理了一遍，下面我们就来把这些零散的东西总结一下 线程池的几种状态线程池一共有5种状态，分别为Running、ShutDown、Stop、Tidying、Terminated Running 当线程池初始化完成之后就为Running状态，此状态可以正常地接收任务并处理任务 ShutDown 当主动调用了shutdown方法之后，若当前状态为Running，则会将状态修改为ShutDown并且shutdown方法会调用 interruptIdleWorkers(false); 此方法的作用是调用当前所有线程的interrupt方法，将当前所有线程标记为中断状态然后又addWork可知，一旦当前状态不是Running的时候，是不会添加新的任务的，因此，ShutDown状态的线程池的状态是：不接受新任务添加，但会处理已有任务 Stop 当主动调用shutdownNow方法后，若当前状态为Running，或者ShutDown，则将当前状态置为Stop此状态与ShutDown的区别在于，shutdownNow方法会移除掉当前workQueue中的所有任务。因此当前状态的线程池的状态是：不接受新任务添加，且不执行未执行的任务 Tidying 当当前线程池中线程数为0，并且workQueue为空时，当前线程池状态为Tidying Terminated 当执行完terminated方法后，线程池的状态会被置为Terminated 线程池的添加策略 若当前线程数小于核心线程数则添加一个Work也就是一个线程 若当前线程数大于等于核心线程数则向workQueue也就是参数中的工作队列中添加一个task 若当前线程数大于等于核心线程数并且workQueue队列已满，则添加一个非核心线程 若当前线程数大于等于最大线程数，则使用定义好的拒绝策略拒绝掉此次任务]]></content>
      <categories>
        <category>JDK</category>
      </categories>
      <tags>
        <tag>日常学习</tag>
        <tag>源码</tag>
        <tag>Threadpoolexecutor</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[统一电话服务]]></title>
    <url>%2F2018%2F06%2F02%2F%E7%BB%9F%E4%B8%80%E7%94%B5%E8%AF%9D%E6%9C%8D%E5%8A%A1%2F</url>
    <content type="text"><![CDATA[统一电话服务模块化改造目标将现有服务按功能拆分为独立模块,内部系统结构改为各个模块之间通信的方式，减少冗余代码，解耦各个模块之间的依赖提高代码可读性和可维护性 模块划分经过对系统功能的梳理，一共划分为了如下5个互相解耦的模块，每个模块大致功能如下 校验及数据转化模块 系统中所有需要进行数据校验的操作，包括参数校验，返回值校验，以及需要进行数据转换的服务，包括16进制转10进制，码表转换等，由此模块对外提供 加解密模块 系统中所有需要进行加解密的操作，由此模块对外提供 存储模块 系统中所有需要进行存储的操作全部由此模块提供，包括缓存，db等，同时内置缓存同步策略，路由策略，降级策略，限流策略等 LeafID生成模块 系统内所有leafID的来源，内部包含预拉取，内存队列优化，熔断降级等策略 监控模块 负责监控系统运行状态，对数据进行分析并告警 模块通信方式目前采取的策略是，每个模块各对外暴露一个接口，再由内部实现类去对各个模块进行操作，其中监控模块采用AOP的方式，捕获其他所有模块的返回值及抛出的异常。 存储模块详细设计在所有模块中，存储模块是逻辑最复杂的一块，细分的话可以分为缓存模块和db模块。 缓存模块缓存模块作为高可用最重要的一环，因此相当重要，故而采取了以下策略，尽可能保证缓存服务的稳定 异地双中心部署 当一个请求到缓存模块时会判断地区信息，调用不同的缓存集群 熔断降级 采取redis、tair双缓存互为备份的方案,默认由redis提供服务，一旦redis出现异常，自动降级到tair 缓存结构优化(主要针对redis) 将string类型调整为hash类型，将key的数量级由亿降至百万，降低存储内存占用 db模块db主要用于保证数据可靠，因此重点在于不能有任何脏数据落库，除此之外还要保证db的高可用，不能被突增流量打垮，因此db模块工作如下： 限流 使用公司内部限流框架对写接口进行限流 分布式锁 落库操作其实是一类compareAndSet的操作，对于一条数据会首先查询db中有没有，没有再落库但是在并发量比较高的情况下多个线程同时写同一条数据的时候，会产生往数据库中插入同一条记录的问题,对于这个问题我们的解决方式是为每个写入操作添加一个分布式锁，过期时间为1s，当写同一条数据时，只有一个线程能获取到锁，未获取到锁的线程则等待其他线程写入，然后查询返回 leafID模块详细设计 维护线程池预拉取数据到db,数据默认从db中获取 当db中获取数据失败时，自动降级到远程调用 获取到的数据先全部缓存到内存队列中，所有外部调用都从队列中取值]]></content>
      <categories>
        <category>工作</category>
      </categories>
      <tags>
        <tag>美团点评</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[JDK实现的动态代理源码分析]]></title>
    <url>%2F2018%2F04%2F26%2FProxy%2F</url>
    <content type="text"><![CDATA[对于懂的含义，应当是在看完之后能够自己实现出来 Spring使用的动态代理一共有两种方式，一种是基于Jdk的proxy类的只支持接口层面的代理，另一种是基于cglib能支持实现类的代理本文只讨论基于jDK的代理 问题 jdk的动态代理是如何实现的 jdk的动态代理为什么只支持接口代理 使用方式TestProxy testProxy = (TestProxy) Proxy.newProxyInstance(TestProxy.class.getClassLoader(), new Class[]{TestProxy.class}, new InvocationHandler() { @Override public Object invoke(Object proxy, Method method, Object[] args) throws Throwable { System.out.println(&quot;invoke method:&quot;+method.getName()+&quot; args:&quot;+args); return null; } }); testProxy.sayBye(); TestProxy.java public interface TestProxy { void sayHi(); void sayBye(); } 通常调用场景就是使用Proxy类的newProxyInstance方法，先来看看方法签名 public static Object newProxyInstance(ClassLoader loader, Class&lt;?&gt;[] interfaces, InvocationHandler h) 参数有3个，含义如下: loader:类加载器，用于将生成的class文件加载进内存(方法区) interfaces:代理类要实现的接口,只支持接口，不支持实现类 h:InvocationHandler最终所有方法的调用都会走到它的invoke方法 实现实现其实很简单，核心逻辑用一句话就可以概括：使用ProxyGenerator.generateProxyClass生成二进制class文件，然后用类加载器加载进内存，最后使用构造器构造实例(ps:当前具体实现是没有这么简单的，具体实现还包括一些安全性校验，以及提高效率使用的二级缓存策略这些,但是这不是本文讨论的重点，因此暂不详细描述) 其中重点当然就是生成的代理类啦，我们在原来的代码加入下面这个一行代码，将生成的class文件保存下来 System.getProperties().put(&quot;sun.misc.ProxyGenerator.saveGeneratedFiles&quot;,&quot;true&quot;); 生成的class文件如下： $Proxy0.class package com.sun.proxy; import java.lang.reflect.InvocationHandler; import java.lang.reflect.Method; import java.lang.reflect.Proxy; import java.lang.reflect.UndeclaredThrowableException; public final class $Proxy0 extends Proxy implements TestProxy { private static Method m1; private static Method m3; private static Method m4; private static Method m0; private static Method m2; public $Proxy0(InvocationHandler var1) throws { super(var1); } public final boolean equals(Object var1) throws { try { return ((Boolean)super.h.invoke(this, m1, new Object[]{var1})).booleanValue(); } catch (RuntimeException | Error var3) { throw var3; } catch (Throwable var4) { throw new UndeclaredThrowableException(var4); } } public final void sayBye() throws { try { super.h.invoke(this, m3, (Object[])null); } catch (RuntimeException | Error var2) { throw var2; } catch (Throwable var3) { throw new UndeclaredThrowableException(var3); } } public final void sayHi() throws { try { super.h.invoke(this, m4, (Object[])null); } catch (RuntimeException | Error var2) { throw var2; } catch (Throwable var3) { throw new UndeclaredThrowableException(var3); } } public final int hashCode() throws { try { return ((Integer)super.h.invoke(this, m0, (Object[])null)).intValue(); } catch (RuntimeException | Error var2) { throw var2; } catch (Throwable var3) { throw new UndeclaredThrowableException(var3); } } public final String toString() throws { try { return (String)super.h.invoke(this, m2, (Object[])null); } catch (RuntimeException | Error var2) { throw var2; } catch (Throwable var3) { throw new UndeclaredThrowableException(var3); } } static { try { m1 = Class.forName(&quot;java.lang.Object&quot;).getMethod(&quot;equals&quot;, Class.forName(&quot;java.lang.Object&quot;)); m3 = Class.forName(&quot;TestProxy&quot;).getMethod(&quot;sayBye&quot;); m4 = Class.forName(&quot;TestProxy&quot;).getMethod(&quot;sayHi&quot;); m0 = Class.forName(&quot;java.lang.Object&quot;).getMethod(&quot;hashCode&quot;); m2 = Class.forName(&quot;java.lang.Object&quot;).getMethod(&quot;toString&quot;); } catch (NoSuchMethodException var2) { throw new NoSuchMethodError(var2.getMessage()); } catch (ClassNotFoundException var3) { throw new NoClassDefFoundError(var3.getMessage()); } } } 可以看到除了TestProxy的两个方法sayHi，sayBy之外还有hashcode,toString,equals 这三个Object类自带的方法至于为什么非要有这三个方法，我暂时也没弄明白….. 不管这个奇怪的逻辑，我们来看sayHi和sayBy 的具体实现，发现调用过程其实是先用反射拿到TestProxy的method然后调用之前传入的InvocationHandler实例的invoke方法，将对应的method和params传入,然后由实例内部的逻辑去处理 结论Q: jdk的动态代理是如何实现的A: 读取传入的interface的method信息，生成实现对应方法的字节码文件，然后具体实现就是调用传入的InvocationHandler的invoke方法 Q: jdk的动态代理为什么只支持接口代理A: 由生成的类结构可以看出，代理类是继承了Proxy类然后实现代理接口的，由于java的单继承策略,所以没办法继续继承实现类 一些思考 为什么生成的代理类要继承Proxy类? 为什么invoke方法中有一个参数是proxy这个可以参考这篇文章 若是要对类进行代理,那么应该怎么做？代理类的实现应该怎么写？]]></content>
      <categories>
        <category>JDK</category>
      </categories>
      <tags>
        <tag>日常学习</tag>
        <tag>源码</tag>
        <tag>proxy</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[一些想法]]></title>
    <url>%2F2018%2F01%2F03%2F%E4%B8%80%E4%BA%9B%E6%83%B3%E6%B3%95%2F</url>
    <content type="text"><![CDATA[我想要的无非是看得见的未来和到得了的远方 过去独身一人，懵懂无知，得过且过 现在虽然还是一条咸鱼，还是那么无能，但是拥有了你，好像明白了很多事情 未来成为自己一直想成为的那个人，不再当一条令人恶心的咸鱼 总结人生路长，风景很多，值得我们付出所有的努力去到更远的地方，看到更好的风景 当然，最重要的是，不论旅途艰辛与否，风景动人还是令人失望，身边始终有你 — kep 于2018年1月3日晚 胡言乱语]]></content>
      <categories>
        <category>杂谈</category>
      </categories>
      <tags>
        <tag>随想</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[咸鱼之家今天正式成立啦]]></title>
    <url>%2F2017%2F11%2F11%2Fhello-world%2F</url>
    <content type="text"><![CDATA[“双11，别人在剁手，我在码代码” 前言在换了几个大众化的博客之后终于想要自己来搭一个博客环境了。 当然作为一个只会html，css，JavaScript等单词拼写的人 写前端是不会写的，只能勉强靠着copy大佬的代码才能勉强有个能看的界面这个样子 于是周末花了一天时间找了一堆大佬们模板，最终选择了用这个看这里 毕竟反正自己不会写，用哪个都是一样的 正文以后打算在这里写一写平时自己学到的新姿势，生活中的一些瞎想等等 欢迎各位基佬前来围观 —— kep 于 2017年 11.11]]></content>
      <categories>
        <category>杂谈</category>
      </categories>
      <tags>
        <tag>随想</tag>
      </tags>
  </entry>
</search>
